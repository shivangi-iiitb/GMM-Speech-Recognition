{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "import os\n",
    "import tables\n",
    "from os import path\n",
    "import sys\n",
    "import tarfile\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from mapping import phone_maps\n",
    "import python_speech_features as psf\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_phone_map = phone_maps(mapping_file=\"kaldi_60_48_39.map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(word):\n",
    "    # LC ALL & strip punctuation which are not required\n",
    "    new = word.lower().replace('.', '')\n",
    "    new = new.replace(',', '')\n",
    "    new = new.replace(';', '')\n",
    "    new = new.replace('\"', '')\n",
    "    new = new.replace('!', '')\n",
    "    new = new.replace('?', '')\n",
    "    new = new.replace(':', '')\n",
    "    new = new.replace('-', '')\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfcc(wav_file, n_delta=0):\n",
    "    mfcc_feat = psf.mfcc(wav_file)\n",
    "    if(n_delta == 0):\n",
    "        return(mfcc_feat)\n",
    "    elif(n_delta == 1):\n",
    "        return(np.hstack((mfcc_feat, psf.delta(mfcc_feat,1))))\n",
    "    elif(n_delta == 2):\n",
    "        return(np.hstack((mfcc_feat, psf.delta(mfcc_feat,1), psf.delta(mfcc_feat, 2))))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transcript(full_wav):\n",
    "    trans_file = full_wav[:-8] + \".PHN\"\n",
    "    with open(trans_file, \"r\") as file:\n",
    "        trans = file.readlines()\n",
    "    durations = [ele.strip().split(\" \")[:-1] for ele in trans]\n",
    "    durations_int = []\n",
    "    for duration in durations:\n",
    "        durations_int.append([int(duration[0]), int(duration[1])])\n",
    "    trans = [ele.strip().split(\" \")[-1] for ele in trans]\n",
    "    trans = [timit_phone_map.map_symbol_reduced(symbol=phoneme) for phoneme in trans]\n",
    "    # trans = \" \".join(trans)\n",
    "    return trans, durations_int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting train dataset sphere files into wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_train_rifwav(args):\n",
    "    target = args[\"datapath\"]\n",
    "    preprocessed = args[\"preprocessed\"]\n",
    "    print(\"Preprocessing data\")\n",
    "    print(preprocessed)\n",
    "    # Assume data is downloaded from LDC - https://catalog.ldc.upenn.edu/ldc93s1\n",
    "    # We convert the .WAV (NIST sphere format) into MSOFT .wav\n",
    "    # creates _rif.wav as the new .wav file\n",
    "    if(preprocessed):\n",
    "        print(\"Train data is already preprocessed, now compute train features\")\n",
    "    else:\n",
    "        print(os.path.join(target, \"TIMIT\\\\TRAIN\"))\n",
    "        file = open('train_wavs', 'a+')\n",
    "        for root, dirnames, filenames in os.walk(os.path.join(target, \"TIMIT\\\\TRAIN\")):\n",
    "            for filename in fnmatch.filter(filenames, \"*.WAV\"):\n",
    "                sph_file = os.path.join(root[35:], filename)\n",
    "                wav_file = os.path.join(root[35:], filename)[:-4] + \"_rif.wav\"\n",
    "                path = os.path.join(root, filename)[:-4] + \"_rif.wav\"\n",
    "                file.write(path+\",\")\n",
    "                print(\"converting {} to {}\".format(sph_file, wav_file))\n",
    "                cmd = \"sph2pipe -f wav \"+ sph_file+\" \"+wav_file\n",
    "                subprocess.call(cmd)\n",
    "        print(\"Preprocessing Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting test dataset sphere files into wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_test_rifwav(args):\n",
    "    target = args[\"datapath\"]\n",
    "    preprocessed = args[\"preprocessed\"]\n",
    "    print(\"Preprocessing data\")\n",
    "    print(preprocessed)\n",
    "    # Assume data is downloaded from LDC - https://catalog.ldc.upenn.edu/ldc93s1\n",
    "    # We convert the .WAV (NIST sphere format) into MSOFT .wav\n",
    "    # creates _rif.wav as the new .wav file\n",
    "    if(preprocessed):\n",
    "        print(\"Test data is already preprocessed, now compute test features\")\n",
    "    else:\n",
    "        print(os.path.join(target, \"TIMIT\\\\TEST\"))\n",
    "        file = open('test_wavs', 'a+')\n",
    "        for root, dirnames, filenames in os.walk(os.path.join(target, \"TIMIT\\\\TEST\")):\n",
    "            for filename in fnmatch.filter(filenames, \"*.WAV\"):\n",
    "                sph_file = os.path.join(root[35:], filename)\n",
    "                wav_file = os.path.join(root[35:], filename)[:-4] + \"_rif.wav\"\n",
    "                path = os.path.join(root, filename)[:-4] + \"_rif.wav\"\n",
    "                file.write(path+\",\")\n",
    "                #wav_file1 = os.path.join(root, filename)[:-4] + \"_rif.wav\"\n",
    "                print(\"converting {} to {}\".format(sph_file, wav_file))\n",
    "                cmd = \"sph2pipe -f wav \"+ sph_file+\" \"+wav_file\n",
    "                subprocess.call(cmd)\n",
    "        print(\"Preprocessing Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing train and test features and dumping it into hdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(args):\n",
    "    n_delta = args[\"n_delta\"]\n",
    "    dataset = args[\"dataset\"]\n",
    "    \n",
    "    print(\"Building CSVs\")\n",
    "\n",
    "    mfcc_features = []\n",
    "    mfcc_labels = []\n",
    "\n",
    "    if(dataset == \"train\"):\n",
    "        with open(\"train_wavs\", \"r\") as file:\n",
    "            full_wavs = file.readlines()\n",
    "        full_wavs = [ele.strip().split(\",\") for ele in full_wavs]\n",
    "        full_wavs = full_wavs[0][:-1]\n",
    "   \n",
    "        for full_wav in full_wavs:\n",
    "            print(\"Computing for file: \", full_wav)\n",
    "            trans, durations = read_transcript(full_wav = full_wav)\n",
    "            n_delta = int(args[\"n_delta\"])\n",
    "            labels = []\n",
    "            (sample_rate,wav_file) = wav.read(full_wav)\n",
    "            mfcc_feats = compute_mfcc(wav_file[durations[0][0]:durations[0][1]], n_delta=n_delta)\n",
    "\n",
    "            for i in range(len(mfcc_feats)):\n",
    "                    labels.append(trans[0])\n",
    "            for index, chunk in enumerate(durations[1:]):\n",
    "                mfcc_feat = compute_mfcc(wav_file[chunk[0]:chunk[1]], n_delta=n_delta)\n",
    "                mfcc_feats = np.vstack((mfcc_feats, mfcc_feat))\n",
    "                for i in range(len(mfcc_feat)):\n",
    "                    labels.append(trans[index])\n",
    "            mfcc_features.extend(mfcc_feats)\n",
    "            mfcc_labels.extend(labels)\n",
    "        #Possibly separate features phone-wise and dump them? (np.where() could be used)\n",
    "        timit_df = pd.DataFrame()\n",
    "        timit_df[\"features\"] = mfcc_features\n",
    "        timit_df[\"labels\"] = mfcc_labels\n",
    "        if n_delta == 0:\n",
    "            timit_df.to_hdf(\"./train_features/mfcc/timit.hdf\", \"timit\")\n",
    "        elif n_delta == 1:\n",
    "            timit_df.to_hdf(\"./train_features/mfcc_delta/timit.hdf\", \"timit\")\n",
    "        elif n_delta == 2:\n",
    "            timit_df.to_hdf(\"./train_features/mfcc_delta_delta/timit.hdf\", \"timit\")\n",
    "        print(\"training features extracted\")\n",
    "    else:\n",
    "        with open(\"test_wavs\", \"r\") as file:\n",
    "            full_wavs = file.readlines()\n",
    "        full_wavs = [ele.strip().split(\",\") for ele in full_wavs]\n",
    "        full_wavs = full_wavs[0][:-1]\n",
    "   \n",
    "        for full_wav in full_wavs:\n",
    "            print(\"Computing for file: \", full_wav)\n",
    "            trans, durations = read_transcript(full_wav = full_wav)\n",
    "            n_delta = int(args[\"n_delta\"])\n",
    "            labels = []\n",
    "            (sample_rate,wav_file) = wav.read(full_wav)\n",
    "            mfcc_feats = compute_mfcc(wav_file[durations[0][0]:durations[0][1]], n_delta=n_delta)\n",
    "\n",
    "            for i in range(len(mfcc_feats)):\n",
    "                    labels.append(trans[0])\n",
    "            for index, chunk in enumerate(durations[1:]):\n",
    "                mfcc_feat = compute_mfcc(wav_file[chunk[0]:chunk[1]], n_delta=n_delta)\n",
    "                mfcc_feats = np.vstack((mfcc_feats, mfcc_feat))\n",
    "                for i in range(len(mfcc_feat)):\n",
    "                    labels.append(trans[index])\n",
    "            mfcc_features.extend(mfcc_feats)\n",
    "            mfcc_labels.extend(labels)\n",
    "        #Possibly separate features phone-wise and dump them? (np.where() could be used)\n",
    "        timit_df = pd.DataFrame()\n",
    "        timit_df[\"features\"] = mfcc_features\n",
    "        timit_df[\"labels\"] = mfcc_labels\n",
    "        if n_delta == 0:\n",
    "            timit_df.to_hdf(\"./test_features/mfcc/timit.hdf\", \"timit\")\n",
    "        elif n_delta == 1:\n",
    "            timit_df.to_hdf(\"./test_features/mfcc_delta/timit.hdf\", \"timit\")\n",
    "        elif n_delta == 2:\n",
    "            timit_df.to_hdf(\"./test_features/mfcc_delta_delta/timit.hdf\", \"timit\")\n",
    "        print(\"testing features extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "True\n",
      "Test data is already preprocessed, now compute test features\n",
      "Preprocessing data\n",
      "True\n",
      "Train data is already preprocessed, now compute train features\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args1 = {\"datapath\": \"C:\\\\Users\\\\Shivangi Singh\\\\ASRproject\",\"preprocessed\": True}\n",
    "    convert_test_rifwav(args1)\n",
    "    convert_train_rifwav(args1)\n",
    "    \n",
    "    arg_train_delta0 = {\"n_delta\" : 0, \"dataset\" : \"train\"}\n",
    "    arg_train_delta1 = {\"n_delta\" : 1, \"dataset\" : \"train\"}\n",
    "    arg_train_delta2 = {\"n_delta\" : 2, \"dataset\" : \"train\"}\n",
    "    \n",
    "    arg_test_delta0 = {\"n_delta\" : 0, \"dataset\" : \"test\"}\n",
    "    arg_test_delta1 = {\"n_delta\" : 1, \"dataset\" : \"test\"}\n",
    "    arg_test_delta2 = {\"n_delta\" : 2, \"dataset\" : \"test\"}\n",
    "    \n",
    "    #already computed features by executing these calls once\n",
    "    #compute_features(arg_train_delta0)\n",
    "    #compute_features(arg_train_delta1)\n",
    "    #compute_features(arg_train_delta2)\n",
    "    #compute_features(arg_test_delta0)\n",
    "    #compute_features(arg_test_delta1)\n",
    "    #compute_features(arg_test_delta2)\n",
    "    print(\"Completed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
